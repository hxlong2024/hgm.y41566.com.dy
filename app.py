import streamlit as st
import os
import time
import re
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
from webdriver_manager.core.os_manager import ChromeType

# --- é¡µé¢é…ç½® ---
st.set_page_config(page_title="äº‘ç«¯ç½‘ç›˜æœ", page_icon="â˜ï¸")
st.title("â˜ï¸ ç½‘ç›˜èµ„æºæœç´¢å™¨ ")

# --- æ ¸å¿ƒçˆ¬è™«å‡½æ•° ---
def get_driver():
    chrome_options = Options()
    
    # Streamlit Cloud éƒ¨ç½²å¿…é¡»çš„è®¾ç½®
    chrome_options.add_argument("--headless")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--disable-gpu")
    
    # ä¼ªè£… UA
    chrome_options.add_argument("user-agent=Mozilla/5.0 (iPhone; CPU iPhone OS 13_2_3 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.0.3 Mobile/15E148 Safari/604.1")
    chrome_options.add_argument("--window-size=375,812")
    
    # æé€Ÿè®¾ç½®
    prefs = {"profile.managed_default_content_settings.images": 2}
    chrome_options.add_experimental_option("prefs", prefs)
    chrome_options.page_load_strategy = 'eager'

    return webdriver.Chrome(
        service=Service(
            ChromeDriverManager(chrome_type=ChromeType.CHROMIUM).install()
        ),
        options=chrome_options
    )

def extract_pwd(text_context):
    """
    è¾…åŠ©å‡½æ•°ï¼šæå–æå–ç 
    å…¼å®¹ä¸­æ–‡å†’å· 'ï¼š' å’Œè‹±æ–‡å†’å· ':'
    """
    # åŒ¹é… "æå–ç " åé¢çš„å†’å·å’Œ4ä½å­—ç¬¦
    match = re.search(r'æå–ç \s*[:ï¼š]\s*([a-zA-Z0-9]{4})', text_context)
    if match:
        return match.group(1)
    return None

def scrape_data(keyword):
    driver = None
    try:
        driver = get_driver()
        url = "http://hgm.y41566.com/app/index.html?id=test"
        driver.get(url)
        
        # 1. ç­‰å¾…è¾“å…¥æ¡†
        wait = WebDriverWait(driver, 10)
        search_input = wait.until(EC.element_to_be_clickable((By.ID, "search")))
        
        # 2. è¾“å…¥
        search_input.clear()
        search_input.send_keys(keyword)
        
        # 3. ç‚¹å‡»
        btn = driver.find_element(By.ID, "submitSearch")
        driver.execute_script("arguments[0].click();", btn)
        
        # 4. ç­‰å¾…ç»“æœ
        # ã€ä¿®æ”¹ç‚¹1ã€‘æ”¹ä¸ºç­‰å¾… .info å‡ºç°ï¼Œå› ä¸ºæœ‰çš„ç»“æœå¯èƒ½æ²¡æœ‰ js-title
        try:
            wait.until(EC.presence_of_element_located((By.CLASS_NAME, "info")))
            time.sleep(0.5)
        except:
            return []
            
        # 5. è§£æ
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        results = []
        boxes = soup.find_all("div", class_="access-box")
        
        for box in boxes:
            # --- ã€ä¿®å¤ç‚¹2ã€‘æ ‡é¢˜æå–é€»è¾‘ ---
            title = ""
            # ç­–ç•¥ A: ä¼˜å…ˆæ‰¾ js-title (æ ‡å‡†æƒ…å†µ)
            title_tag = box.find(class_="js-title")
            if title_tag:
                title = title_tag.get_text(strip=True)
            
            # ç­–ç•¥ B: å¦‚æœæ²¡æœ‰ js-titleï¼Œå» info é‡Œæ‰¾ç¬¬ä¸€æ®µæ–‡æœ¬ (ä½ é‡åˆ°çš„æƒ…å†µ)
            info_div = box.find("div", class_="info")
            if not title and info_div:
                # stripped_strings è·å–æ‰€æœ‰éæ ‡ç­¾æ–‡æœ¬
                for text in info_div.stripped_strings:
                    # æ’é™¤æ‰ "é“¾æ¥ï¼š" "æå–ç ï¼š" ç­‰åŠŸèƒ½æ€§æ–‡å­—ï¼Œå‰©ä¸‹çš„é•¿æ–‡æœ¬å°±æ˜¯æ ‡é¢˜
                    if "é“¾æ¥" not in text and "æå–ç " not in text and len(text) > 1:
                        # ã€å…³é”®ã€‘å»æ‰ä¸¤è¾¹çš„å¼•å· "
                        title = text.strip('"').strip()
                        break
            
            if not title:
                title = "æœªçŸ¥èµ„æº" # å…œåº•

            baidu_data = None
            quark_data = None
            
            # --- ã€ä¿®å¤ç‚¹3ã€‘è·å–å®Œæ•´æ–‡æœ¬ç”¨äºæŸ¥æ‰¾æå–ç  ---
            full_text_context = ""
            if info_div:
                visible_text = info_div.get_text(separator=" ", strip=True)
                # è·å–æŒ‰é’®é‡Œçš„éšè—æ–‡æœ¬
                copy_btn = info_div.find("button", class_="js-copy")
                clipboard_text = copy_btn.get("data-clipboard-text", "") if copy_btn else ""
                full_text_context = visible_text + " " + clipboard_text

            # --- ã€ä¿®å¤ç‚¹4ã€‘æ­£åˆ™å‡çº§ ---
            # åŸæ¥çš„æ­£åˆ™é‡åˆ° ? å°±åœäº†ï¼Œç°åœ¨æ”¹ä¸ºåŒ¹é…ç›´åˆ°é‡åˆ°ç©ºæ ¼æˆ–å¼•å·ï¼Œèƒ½åŒ¹é…å®Œæ•´é“¾æ¥
            all_links = re.findall(r'(https?://(?:pan\.baidu\.com|pan\.quark\.cn|pan\.xunlei\.com)[^\s"<>]+)', full_text_context)
            
            # æå–å¯†ç 
            pwd = extract_pwd(full_text_context)

            for link in all_links:
                if "baidu.com" in link:
                    # å¦‚æœæ‰¾åˆ°äº†å¯†ç ä¸” URL é‡Œæ²¡æœ‰ pwd å‚æ•°ï¼Œè‡ªåŠ¨æ‹¼æ¥ä¸Šå»
                    final_url = link
                    if pwd and "pwd=" not in link:
                        connector = "&" if "?" in link else "?"
                        final_url = f"{link}{connector}pwd={pwd}"
                    
                    baidu_data = {"url": final_url, "pwd": pwd}
                    
                elif "quark.cn" in link:
                    quark_data = {"url": link, "pwd": None} 
            
            if baidu_data or quark_data:
                results.append({"title": title, "baidu": baidu_data, "quark": quark_data})
                
        return results
        
    except Exception as e:
        st.error(f"è¿è¡Œå‡ºé”™: {e}")
        return []
    finally:
        if driver:
            driver.quit()

# --- ç•Œé¢ ---
query = st.text_input("è¯·è¾“å…¥æœç´¢å…³é”®è¯")
if st.button("æœç´¢"):
    if query:
        with st.spinner("äº‘ç«¯æœåŠ¡å™¨æ­£åœ¨æœç´¢..."):
            data = scrape_data(query)
            if data:
                st.success(f"æ‰¾åˆ° {len(data)} ä¸ªç»“æœ")
                for item in data:
                    with st.container(border=True):
                        st.write(f"ğŸ¬ **{item['title']}**")
                        
                        cols = st.columns(2)
                        
                        # ç™¾åº¦ç½‘ç›˜åˆ—
                        with cols[0]:
                            if item['baidu']: 
                                url = item['baidu']['url']
                                pwd = item['baidu']['pwd']
                                label = f"[ç™¾åº¦ç½‘ç›˜]({url})"
                                # å¦‚æœæœ‰æå–ç ï¼Œæ˜¾ç¤ºåœ¨é“¾æ¥æ—è¾¹
                                if pwd:
                                    label += f" (ç : `{pwd}`)"
                                st.markdown(label)
                            else:
                                st.caption("æ— ç™¾åº¦èµ„æº")

                        # å¤¸å…‹ç½‘ç›˜åˆ—
                        with cols[1]:
                            if item['quark']: 
                                st.markdown(f"[å¤¸å…‹ç½‘ç›˜]({item['quark']['url']})")
                            else:
                                st.caption("æ— å¤¸å…‹èµ„æº")
            else:
                st.warning("æœªæ‰¾åˆ°ç»“æœ (å¯èƒ½æ˜¯äº‘ç«¯IPè¢«ç›®æ ‡ç½‘ç«™å±è”½ï¼Œæˆ–è€…ç¡®å®æ²¡èµ„æº)")
    else:
        st.warning("è¯·è¾“å…¥å†…å®¹")
