import streamlit as st
import os
import time
import re
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
from webdriver_manager.core.os_manager import ChromeType

# --- é¡µé¢é…ç½® ---
st.set_page_config(page_title="äº‘ç«¯ç½‘ç›˜æœ", page_icon="â˜ï¸", layout="wide")
st.title("â˜ï¸ ç½‘ç›˜èµ„æºæœç´¢å™¨ (ä¿®å¤ç‰ˆ)")

# --- æ ¸å¿ƒçˆ¬è™«å‡½æ•° ---
def get_driver():
    chrome_options = Options()
    # Streamlit Cloud éƒ¨ç½²å¿…é¡»çš„è®¾ç½®
    chrome_options.add_argument("--headless")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--disable-gpu")
    # ä¼ªè£… UA
    chrome_options.add_argument("user-agent=Mozilla/5.0 (iPhone; CPU iPhone OS 13_2_3 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.0.3 Mobile/15E148 Safari/604.1")
    chrome_options.add_argument("--window-size=375,812")
    # æé€Ÿè®¾ç½®
    prefs = {"profile.managed_default_content_settings.images": 2}
    chrome_options.add_experimental_option("prefs", prefs)
    chrome_options.page_load_strategy = 'eager'

    return webdriver.Chrome(
        service=Service(ChromeDriverManager(chrome_type=ChromeType.CHROMIUM).install()),
        options=chrome_options
    )

def extract_pwd(url, text_context):
    """
    ä¸“é—¨æå–æå–ç çš„è¾…åŠ©å‡½æ•°
    """
    pwd = ""
    # 1. æ£€æŸ¥ URL å‚æ•°
    url_pwd_match = re.search(r'[?&]pwd=([a-zA-Z0-9]+)', url)
    if url_pwd_match:
        return url_pwd_match.group(1)

    # 2. æ£€æŸ¥æ–‡æœ¬ä¸Šä¸‹æ–‡ (æ”¯æŒä¸­æ–‡å†’å·å’Œè‹±æ–‡å†’å·)
    # é€»è¾‘ï¼šåœ¨å…¨éƒ¨æ–‡æœ¬ä¸­æ‰¾ "æå–ç " åé¢ç´§è·Ÿç€çš„ 4 ä½å­—ç¬¦
    # ä¹Ÿå°±æ˜¯ä¸ç”¨ç®¡å®ƒæ˜¯ä¸æ˜¯ç´§æŒ¨ç€é“¾æ¥ï¼Œåªè¦è¿™æ®µè¯é‡Œæœ‰æå–ç ï¼Œå°±æŠ“å‡ºæ¥
    text_pwd_match = re.search(r'æå–ç \s*[:ï¼š]\s*([a-zA-Z0-9]{4})', text_context)
    if text_pwd_match:
        pwd = text_pwd_match.group(1)
    
    return pwd

def scrape_data(keyword):
    driver = None
    try:
        driver = get_driver()
        url = "http://hgm.y41566.com/app/index.html?id=test"
        driver.get(url)
        
        # 1. ç­‰å¾…è¾“å…¥æ¡†
        wait = WebDriverWait(driver, 10)
        search_input = wait.until(EC.element_to_be_clickable((By.ID, "search")))
        
        # 2. è¾“å…¥
        search_input.clear()
        search_input.send_keys(keyword)
        
        # 3. ç‚¹å‡»
        btn = driver.find_element(By.ID, "submitSearch")
        driver.execute_script("arguments[0].click();", btn)
        
        # 4. ç­‰å¾…ç»“æœ
        try:
            # åªè¦ .info å‡ºç°å°±å¯ä»¥ï¼Œå› ä¸ºæœ‰çš„ç»“æœæ²¡æœ‰ .js-title
            wait.until(EC.presence_of_element_located((By.CLASS_NAME, "info")))
            time.sleep(0.5)
        except:
            return []
            
        # 5. è§£æ
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        results = []
        boxes = soup.find_all("div", class_="access-box")
        
        for box in boxes:
            # --- ã€ä¿®å¤ã€‘æ ‡é¢˜è·å–é€»è¾‘ ---
            title = ""
            
            # æ–¹æ¡ˆA: å°è¯•æ‰¾æ ‡å‡†çš„ js-title
            title_tag = box.find(class_="js-title")
            if title_tag:
                title = title_tag.get_text(strip=True)
            
            # æ–¹æ¡ˆB: å¦‚æœæ²¡æœ‰ js-titleï¼Œå» info é‡Œæ‰¾ç¬¬ä¸€æ®µæ–‡æœ¬
            info_div = box.find("div", class_="info")
            if not title and info_div:
                # stripped_strings ä¼šç”Ÿæˆæ‰€æœ‰éæ ‡ç­¾çš„çº¯æ–‡æœ¬
                # æˆ‘ä»¬å–ç¬¬ä¸€ä¸ªä¸æ˜¯ "é“¾æ¥" ä¹Ÿä¸æ˜¯ "æå–ç " çš„æ–‡æœ¬ä½œä¸ºæ ‡é¢˜
                for text in info_div.stripped_strings:
                    if "é“¾æ¥" not in text and "æå–ç " not in text and len(text) > 1:
                        # å»æ‰å¯èƒ½å­˜åœ¨çš„å¼•å·
                        title = text.strip('"').strip()
                        break
            
            if not title:
                title = "æœªçŸ¥èµ„æº (ç‚¹å‡»é“¾æ¥æŸ¥çœ‹)"

            baidu_data = None
            quark_data = None
            
            # --- è·å–æ‰€æœ‰ç›¸å…³æ–‡æœ¬ç”¨äºæŸ¥æ‰¾æå–ç  ---
            full_text_context = ""
            if info_div:
                # å¯è§æ–‡æœ¬
                visible_text = info_div.get_text(separator=" ", strip=True)
                # éšè—åœ¨æŒ‰é’®é‡Œçš„æ–‡æœ¬
                copy_btn = info_div.find("button", class_="js-copy")
                clipboard_text = copy_btn.get("data-clipboard-text", "") if copy_btn else ""
                full_text_context = visible_text + " " + clipboard_text

            # --- é“¾æ¥åŒ¹é… ---
            all_links = re.findall(r'(https?://(?:pan\.baidu\.com|pan\.quark\.cn|pan\.xunlei\.com)[^\s"<>]+)', full_text_context)
            
            for link in all_links:
                # æå–å¯†ç 
                pwd = extract_pwd(link, full_text_context)
                
                if "baidu.com" in link:
                    # è‡ªåŠ¨æŠŠæå–ç æ‹¼æ¥åˆ° URL åé¢ (å¦‚æœ URL æœ¬èº«æ²¡æœ‰çš„è¯)
                    final_url = link
                    if pwd and "pwd=" not in link:
                        if "?" in link:
                            final_url = f"{link}&pwd={pwd}"
                        else:
                            final_url = f"{link}?pwd={pwd}"
                    
                    baidu_data = {"url": final_url, "pwd": pwd}
                    
                elif "quark.cn" in link:
                    quark_data = {"url": link, "pwd": pwd}

            # åªæœ‰æœ‰é“¾æ¥æ‰æ˜¾ç¤º
            if baidu_data or quark_data:
                results.append({"title": title, "baidu": baidu_data, "quark": quark_data})
                
        return results
        
    except Exception as e:
        st.error(f"è¿è¡Œå‡ºé”™: {e}")
        return []
    finally:
        if driver:
            driver.quit()

# --- ç•Œé¢ ---
query = st.text_input("è¯·è¾“å…¥æœç´¢å…³é”®è¯", "å–œç¾Šç¾Š")
if st.button("æœç´¢"):
    if query:
        with st.spinner("äº‘ç«¯æœåŠ¡å™¨æ­£åœ¨æœç´¢..."):
            data = scrape_data(query)
            if data:
                st.success(f"æ‰¾åˆ° {len(data)} ä¸ªç»“æœ")
                for item in data:
                    with st.container(border=True):
                        # æ ‡é¢˜åŠ å¤§åŠ ç²—
                        st.markdown(f"### {item['title']}")
                        
                        cols = st.columns(2)
                        with cols[0]:
                            if item['baidu']: 
                                url = item['baidu']['url']
                                pwd = item['baidu']['pwd']
                                # æŒ‰é’®æ–‡æ¡ˆå¸¦ä¸Šæå–ç ï¼Œæ–¹ä¾¿æŸ¥çœ‹
                                btn_label = f"ğŸ‘‰ ç™¾åº¦ç½‘ç›˜ (ç : {pwd})" if pwd else "ğŸ‘‰ ç™¾åº¦ç½‘ç›˜"
                                st.link_button(btn_label, url)
                            else:
                                st.caption("æ— ç™¾åº¦èµ„æº")

                        with cols[1]:
                            if item['quark']: 
                                url = item['quark']['url']
                                st.link_button("ğŸ‘‰ å¤¸å…‹ç½‘ç›˜", url)
                            else:
                                st.caption("æ— å¤¸å…‹èµ„æº")
            else:
                st.warning("æœªæ‰¾åˆ°ç»“æœ")
    else:
        st.warning("è¯·è¾“å…¥å†…å®¹")
