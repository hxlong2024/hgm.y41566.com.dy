import streamlit as st
import os
import time
import re
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
from webdriver_manager.core.os_manager import ChromeType

# --- é¡µé¢é…ç½® ---
st.set_page_config(page_title="äº‘ç«¯ç½‘ç›˜æœ", page_icon="â˜ï¸")
st.title("â˜ï¸ ç½‘ç›˜èµ„æºæœç´¢å™¨ (Cloudç‰ˆ)")

# --- æ ¸å¿ƒçˆ¬è™«å‡½æ•° ---
def get_driver():
    chrome_options = Options()
    
    # ------------------------------------------
    # Streamlit Cloud éƒ¨ç½²å¿…é¡»çš„è®¾ç½®
    # ------------------------------------------
    chrome_options.add_argument("--headless")  # å¿…é¡»æ— å¤´
    chrome_options.add_argument("--no-sandbox") # å¿…é¡»ç¦ç”¨æ²™ç›’
    chrome_options.add_argument("--disable-dev-shm-usage") # è§£å†³å†…å­˜ä¸è¶³
    chrome_options.add_argument("--disable-gpu")
    
    # ä¼ªè£… UA
    chrome_options.add_argument("user-agent=Mozilla/5.0 (iPhone; CPU iPhone OS 13_2_3 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.0.3 Mobile/15E148 Safari/604.1")
    chrome_options.add_argument("--window-size=375,812")
    
    # æé€Ÿè®¾ç½®
    prefs = {"profile.managed_default_content_settings.images": 2}
    chrome_options.add_experimental_option("prefs", prefs)
    chrome_options.page_load_strategy = 'eager'

    # ä½¿ç”¨ webdriver_manager å®‰è£…é€‚åˆ Linux çš„ Chromium é©±åŠ¨
    return webdriver.Chrome(
        service=Service(
            ChromeDriverManager(chrome_type=ChromeType.CHROMIUM).install()
        ),
        options=chrome_options
    )

def extract_pwd(text_context):
    """
    è¾…åŠ©å‡½æ•°ï¼šä»æ–‡æœ¬ä¸­æå–æå–ç 
    æ”¯æŒä¸­æ–‡å†’å· 'ï¼š' å’Œè‹±æ–‡å†’å· ':'
    """
    match = re.search(r'æå–ç \s*[:ï¼š]\s*([a-zA-Z0-9]{4})', text_context)
    if match:
        return match.group(1)
    return None

def scrape_data(keyword):
    driver = None
    try:
        driver = get_driver()
        url = "http://hgm.y41566.com/app/index.html?id=test"
        driver.get(url)
        
        # 1. ç­‰å¾…è¾“å…¥æ¡†
        wait = WebDriverWait(driver, 10)
        search_input = wait.until(EC.element_to_be_clickable((By.ID, "search")))
        
        # 2. è¾“å…¥
        search_input.clear()
        search_input.send_keys(keyword)
        
        # 3. ç‚¹å‡»
        btn = driver.find_element(By.ID, "submitSearch")
        driver.execute_script("arguments[0].click();", btn)
        
        # 4. ç­‰å¾…ç»“æœ
        # æ³¨æ„ï¼šè¿™é‡Œæ”¹ä¸ºç­‰å¾… .info å‡ºç°ï¼Œå› ä¸ºæœ‰çš„ç»“æœæ²¡æœ‰ .js-title
        try:
            wait.until(EC.presence_of_element_located((By.CLASS_NAME, "info")))
            time.sleep(0.5)
        except:
            return []
            
        # 5. è§£æ
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        results = []
        boxes = soup.find_all("div", class_="access-box")
        
        for box in boxes:
            # --- ä¿®å¤æ ‡é¢˜æå–é€»è¾‘ ---
            title = ""
            # æ–¹æ¡ˆ A: å°è¯•è·å–æ ‡å‡†çš„ js-title
            title_tag = box.find(class_="js-title")
            if title_tag:
                title = title_tag.get_text(strip=True)
            
            # æ–¹æ¡ˆ B: å¦‚æœæ²¡æœ‰ js-titleï¼Œå» info é‡Œæ‰¾ç¬¬ä¸€æ®µéé“¾æ¥çš„æ–‡æœ¬
            info_div = box.find("div", class_="info")
            if not title and info_div:
                # stripped_strings ä¼šæå–æ‰€æœ‰éæ ‡ç­¾çš„çº¯æ–‡æœ¬
                for text in info_div.stripped_strings:
                    # è·³è¿‡å…¸å‹çš„æ ‡ç­¾æ–‡å­—ï¼Œæ‰¾åˆ°çœŸæ­£çš„æ ‡é¢˜
                    if "é“¾æ¥" not in text and "æå–ç " not in text and len(text) > 1:
                        # å»æ‰å¯èƒ½å­˜åœ¨çš„å¼•å·
                        title = text.strip('"').strip()
                        break
            
            if not title:
                title = "æœªçŸ¥èµ„æº"

            # å‡†å¤‡æ•°æ®å®¹å™¨
            baidu_data = None
            quark_data = None
            
            # --- è·å–å®Œæ•´æ–‡æœ¬ä¸Šä¸‹æ–‡ç”¨äºæŸ¥æ‰¾æå–ç  ---
            full_text_context = ""
            if info_div:
                visible_text = info_div.get_text(separator=" ", strip=True)
                copy_btn = info_div.find("button", class_="js-copy")
                clipboard_text = copy_btn.get("data-clipboard-text", "") if copy_btn else ""
                full_text_context = visible_text + " " + clipboard_text

            # --- æå–é“¾æ¥ ---
            # æ­£åˆ™åŒ¹é…å®Œæ•´ URLï¼ŒåŒ…æ‹¬å¯èƒ½å­˜åœ¨çš„å‚æ•°
            all_links = re.findall(r'(https?://(?:pan\.baidu\.com|pan\.quark\.cn|pan\.xunlei\.com)[^\s"<>]+)', full_text_context)
            
            # å°è¯•ä»æ–‡æœ¬ä¸­æå–å¯†ç 
            pwd = extract_pwd(full_text_context)

            for link in all_links:
                if "baidu.com" in link:
                    # å¦‚æœæ‰¾åˆ°äº†å¯†ç ä¸” URL é‡Œæ²¡æœ‰ pwd å‚æ•°ï¼Œè‡ªåŠ¨æ‹¼æ¥ä¸Šå»
                    final_url = link
                    if pwd and "pwd=" not in link:
                        connector = "&" if "?" in link else "?"
                        final_url = f"{link}{connector}pwd={pwd}"
                    
                    baidu_data = {"url": final_url, "pwd": pwd}
                    
                elif "quark.cn" in link:
                    quark_data = {"url": link, "pwd": None} # å¤¸å…‹é€šå¸¸ä¸éœ€è¦æå–ç 
            
            # åªæœ‰å½“è‡³å°‘æœ‰ä¸€ä¸ªæœ‰æ•ˆé“¾æ¥æ—¶æ‰æ·»åŠ ç»“æœ
            if baidu_data or quark_data:
                results.append({"title": title, "baidu": baidu_data, "quark": quark_data})
                
        return results
        
    except Exception as e:
        st.error(f"è¿è¡Œå‡ºé”™: {e}")
        return []
    finally:
        if driver:
            driver.quit()

# --- ç•Œé¢ ---
query = st.text_input("è¯·è¾“å…¥æœç´¢å…³é”®è¯")
if st.button("æœç´¢"):
    if query:
        with st.spinner("äº‘ç«¯æœåŠ¡å™¨æ­£åœ¨æœç´¢..."):
            data = scrape_data(query)
            if data:
                st.success(f"æ‰¾åˆ° {len(data)} ä¸ªç»“æœ")
                for item in data:
                    with st.container(border=True):
                        st.write(f"ğŸ¬ **{item['title']}**")
                        
                        cols = st.columns(2)
                        
                        # ç™¾åº¦ç½‘ç›˜åˆ—
                        with cols[0]:
                            if item['baidu']: 
                                url = item['baidu']['url']
                                pwd = item['baidu']['pwd']
                                label = f"[ç™¾åº¦ç½‘ç›˜]({url})"
                                if pwd:
                                    label += f" (ç : `{pwd}`)"
                                st.markdown(label)
                            else:
                                st.caption("æ— ç™¾åº¦èµ„æº")

                        # å¤¸å…‹ç½‘ç›˜åˆ—
                        with cols[1]:
                            if item['quark']: 
                                st.markdown(f"[å¤¸å…‹ç½‘ç›˜]({item['quark']['url']})")
                            else:
                                st.caption("æ— å¤¸å…‹èµ„æº")
            else:
                st.warning("æœªæ‰¾åˆ°ç»“æœ (å¯èƒ½æ˜¯äº‘ç«¯IPè¢«ç›®æ ‡ç½‘ç«™å±è”½ï¼Œæˆ–è€…ç¡®å®æ²¡èµ„æº)")
    else:
        st.warning("è¯·è¾“å…¥å†…å®¹")
